{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process xmls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is going to show you one way to process xmls, extractiong all paragraphs and save them into pickle object for later use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "python_root = '..'\n",
    "import sys\n",
    "sys.path.insert(0, python_root)\n",
    "import data_util\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download some sample data if you don't have them yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### specify download path and extract path \n",
    "download_path = \"staff_reports.zip\"\n",
    "download_link = \"https://www.dropbox.com/s/wi37fy1apjuiyqt/staff_reports.zip?dl=1\"\n",
    "extract_path = \"../data\"  # place data in Python project root folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## detailed of the download_data function is in data_util module in python_root \n",
    "data_util.download_data(download_path,download_link,extract_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Now we can start process xmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_path = \"../data/xmls\"\n",
    "files = os.listdir(xml_path)             ## list all files in xml_path \n",
    "files = [f for f in files if '_' in f]   ## only keep the files with _ in its name \n",
    "                                         ## it is just our xmls are formated that way, \n",
    "                                         ## files without \"_\" are just headers wich don't contrain \n",
    "                                         ## any information        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "Here, we will use beautiful soup package to read xml files, we first define a function to read xml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_xml(file_path):\n",
    "    with open(file_path,'r',encoding='utf8') as f:\n",
    "        soup = BeautifulSoup(f, 'xml')\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try see what the result looks like\n",
    "res = read_xml(os.path.join(xml_path,files[0]))\n",
    "print(res.contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some basic orerations with the xml file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all paragraphs \n",
    "paras = res.body.find_all('p')\n",
    "print(paras[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all tables \n",
    "figs = res.body.find_all('table-wrap')\n",
    "print(figs[0].title)  ## only print out the title of that figure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for more detailed beautiful soup operations, please look at the documentation: \n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Just to make the file a bit cleaner, i made document object in data_util file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- document object takes 3 argument: series_id,file_id,xml_path. \n",
    "- it will reture an object, with a couple of filed: series_id, file_id, paras, meta\n",
    "- the way our xmls are names follows: [xxxxx]-[xxxxxxxxx]_A[xxx].xml \n",
    "- the first part is the series id, second part is the document id. we will extract them out so that we can use those ids to find extrac mata data in the mata data sheet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ids(xml):\n",
    "    \"\"\"\n",
    "    input  :xml full name \n",
    "    return :series id and file id \n",
    "    \"\"\"\n",
    "    series_id,xml_name = xml.split('-')\n",
    "    file_id,_ = xml_name.split('_') \n",
    "    return series_id,file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = get_ids(files[0])\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test = data_util.document(ids[0],ids[1],os.path.join(xml_path,files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test.series_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test.file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test.paras[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Now we will loop through all xml files we have the make them into document object for later use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict = dict()\n",
    "total_length = len(files)\n",
    "print('converting {} xmls into doctment object ......'.format(total_length))\n",
    "for idx,file_name in enumerate(files):\n",
    "    f_path = os.path.join(xml_path,file_name)\n",
    "    try:\n",
    "        series_id,file_id = get_ids(file_name)\n",
    "    except:\n",
    "        print(\"file name is not consistent: \", file_name)\n",
    "        continue\n",
    "    \n",
    "    doc = data_util.document(series_id,file_id,f_path)\n",
    "    try:\n",
    "        if doc.file_id in doc_dict.keys():\n",
    "            doc_dict[doc.file_id].paras.extend(doc.paras)\n",
    "        else:\n",
    "            doc_dict[doc.file_id] = doc\n",
    "    except:\n",
    "        print(doc.file_id)\n",
    "        \n",
    "    #docs_dict[doc.file_id] = doc\n",
    "    if (idx+1)%100 == 0:\n",
    "        print('{} / {} '.format(idx+1,total_length))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let get on sample result \n",
    "print(doc_dict['9781451800203'].file_id)\n",
    "print(doc_dict['9781451800203'].series_id)\n",
    "print(doc_dict['9781451800203'].paras[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Now we save our processed data into python pickle file, so that we can read and write into it easily later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save our doct_dict object inot a pickle file \n",
    "pickle.dump(doc_dict,open(os.path.join(extract_path,'processed_xml.p'), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## we can read it back from pickle\n",
    "doc_dict_2 = pickle.load(open(os.path.join(extract_path,'processed_xml.p'), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict_2['9781451800203'].paras[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now with clean text data, you can move on to search and analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
