{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    "\n",
    "In this notebook, we will introduce word 2 vect algorithm\n",
    "\n",
    "## Readings\n",
    "\n",
    "Here are the resources I used to build this notebook. I suggest reading these either beforehand or while you're working on this material.\n",
    "\n",
    "* A really good [conceptual overview](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) of word2vec from Chris McCormick \n",
    "* [First word2vec paper](https://arxiv.org/pdf/1301.3781.pdf) from Mikolov et al.\n",
    "* [NIPS paper](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) with improvements for word2vec also from Mikolov et al.\n",
    "* An [implementation of word2vec](http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/) from Thushan Ganegedara\n",
    "* TensorFlow [word2vec tutorial](https://www.tensorflow.org/tutorials/word2vec)\n",
    "* Gensim w2v totorial https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/online_w2v_tutorial.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "The word2vec algorithm finds much more efficient representations by finding vectors that represent the words. These vectors also contain semantic information about the words. Words that show up in similar contexts, such as \"black\", \"white\", and \"red\" will have vectors near each other. There are two architectures for implementing word2vec, CBOW (Continuous Bag-Of-Words) and Skip-gram.\n",
    "\n",
    "<img src=\"assets/word2vec_architectures.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "\n",
    "You will have to define the window size of surrounding words that you want to look at. And then, we go through each word in your corpus and create paris of input and output words as training data.\n",
    "\n",
    "<img src=\"assets/rolling_window.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the graph\n",
    "\n",
    "From [Chris McCormick's blog](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/), we can see the general structure of our network.\n",
    "![embedding_network](./assets/skip_gram_net_arch.png)\n",
    "\n",
    "The input words are passed in as integers. This will go into a hidden layer of linear units, then into a softmax layer. We'll use the softmax layer to make a prediction like normal.\n",
    "\n",
    "The idea here is to train the hidden layer weight matrix to find efficient representations for our words. We can discard the softmax layer becuase we don't really care about making predictions with this network. We just want the embedding matrix so we can use it in other networks we build from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lookup](assets/lookup_matrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## input will be a list of tokenized sentances\n",
    "# e.g input = [['good','morning','!'],['how','are','you','?']]\n",
    "# I am just going to read a preprocessed piclke file with all my training sentances.\n",
    "# you will have to use your own data sources\n",
    "total_results = pickle.load(open(\"sentances.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### initialize model and build vocabulary \n",
    "n_dim = 300\n",
    "window = 7 \n",
    "downsampling = 0.001\n",
    "seed = 1 \n",
    "num_workers = os.cpu_count()    ## not sure if this is a good idea\n",
    "min_count = 30 \n",
    "imf_w2v = Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=n_dim,\n",
    "    min_count=min_count,\n",
    "    window= window,\n",
    "    sample=downsampling\n",
    ")\n",
    "## build the vocabulary\n",
    "imf_w2v.build_vocab(total_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## train w2v model \n",
    "corpus_count = imf_w2v.corpus_count\n",
    "iteration = 10\n",
    "if gensim.__version__[0] =='1':\n",
    "    imf_w2v.train(total_results)\n",
    "else:\n",
    "    imf_w2v.train(total_results,total_examples=corpus_count,epochs = iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save trained word2 to vect model \n",
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")\n",
    "    imf_w2v.save(os.path.join('trained','imf.w2v'))\n",
    "else:\n",
    "    imf_w2v = Word2Vec.load(os.path.join('trained','imf.w2v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = imf_w2v.wv\n",
    "vocabs = model.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spillovers', 0.7708446383476257),\n",
       " ('linkages', 0.5720889568328857),\n",
       " ('spill-over', 0.5676941871643066),\n",
       " ('contagion', 0.5501283407211304),\n",
       " ('spillbacks', 0.5454457998275757),\n",
       " ('outwards', 0.5422704219818115),\n",
       " ('marketvolatility', 0.5338668823242188),\n",
       " ('reportâ€”analytical', 0.5317055583000183),\n",
       " ('inwards', 0.5228022336959839),\n",
       " ('outward', 0.5054793953895569),\n",
       " ('reportthe', 0.5010243654251099),\n",
       " ('boomerang', 0.4919307827949524),\n",
       " ('outwardspillovers', 0.4914931058883667),\n",
       " ('inward', 0.4884394407272339),\n",
       " ('effects', 0.4873153269290924),\n",
       " ('spilloverswe', 0.48556917905807495),\n",
       " ('interconnectedness', 0.47779250144958496),\n",
       " ('spillback', 0.4770641624927521),\n",
       " ('interconnections', 0.47506511211395264),\n",
       " ('surveillance.the', 0.47185012698173523)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imf_w2v.most_similar('spillover',topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('npls', 0.8256365656852722),\n",
       " ('nonperforming', 0.7252997756004333),\n",
       " ('non-performing', 0.7176951766014099),\n",
       " ('provisioning', 0.5830786228179932),\n",
       " ('write-offs', 0.5457184314727783),\n",
       " ('impairments', 0.517227292060852),\n",
       " ('loan-loss', 0.5097397565841675),\n",
       " ('impaired', 0.5086087584495544),\n",
       " ('capitaladequacy', 0.5065985918045044),\n",
       " ('loan-to-deposit', 0.497059166431427),\n",
       " ('nonperformingloans', 0.4962959885597229),\n",
       " ('npes', 0.4939619302749634),\n",
       " ('provisions-to-npls', 0.4868103265762329),\n",
       " ('corporatedebt', 0.4835938811302185),\n",
       " ('innon-performing', 0.48146674036979675),\n",
       " ('cost-to-income', 0.47889918088912964),\n",
       " ('loan', 0.47880086302757263),\n",
       " ('loans', 0.4780082106590271),\n",
       " ('cesees', 0.47704681754112244),\n",
       " ('capital-to-assets', 0.4730897843837738)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imf_w2v.most_similar('npl',topn=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
