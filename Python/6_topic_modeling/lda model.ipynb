{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file adds result anaysis and visualization into a tuned LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some terminologies:\n",
    "1. raw_doc: unprocessed raw document from txt file\n",
    "2. docs: lemmentized corpus\n",
    "3. corpus_bow: bag of words corpus\n",
    "4. corpus_tfidf: tfidf corpus\n",
    "\n",
    "#### Change from eariler version:\n",
    "1. filter out documents with too few words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dictionary and pre-built functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel \n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import gensim\n",
    "import pickle\n",
    "#from collections import Counter\n",
    "#import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## global folder path \n",
    "data_folder = '../../data/'\n",
    "raw_data_path = os.path.join(data_folder,'raw/article_IV_corpus.txt')\n",
    "data_processed_folder = os.path.join(data_folder,'processed')\n",
    "results_folder = os.path.join(data_folder,'results','temp_results')\n",
    "## binary file for mallet model\n",
    "mallet_path = '/mnt/packages/Mallet/bin/mallet' # update this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load topic_models.py\n",
    "# python_root = './scripts'\n",
    "# sys.path.insert(0, python_root)\n",
    "\n",
    "#%%\n",
    "def prepare_data(data_folder,save=True):\n",
    "    ## read and transform data \n",
    "    contents = pickle.load(open(os.path.join(data_folder,'lemma_corpus.p'), \"rb\"))\n",
    "    print('length of lemmentized corpus: {}'.format(len(contents)))\n",
    "    docs = list()\n",
    "    for paragraph in contents:\n",
    "        docs.append([w for sentance in paragraph for w in sentance])\n",
    "\n",
    "    # build dictionary\n",
    "\n",
    "    dictionary = corpora.Dictionary(docs)\n",
    "    dictionary.filter_extremes(no_below=5,no_above=0.5, keep_n=10000)\n",
    "    # convert document into bow\n",
    "    corpus_bow = [dictionary.doc2bow(text) for text in docs]\n",
    "    ## comput tfidf feature vectors\n",
    "    tfidf = models.TfidfModel(corpus_bow) # smartirs = 'atc' https://radimrehurek.com/gensim/models/tfidfmodel.html\n",
    "    corpus_tfidf = tfidf[corpus_bow]\n",
    "    \n",
    "    ## save dictionary and corpora \n",
    "    if save:\n",
    "        dictionary_save_path = os.path.join(data_folder,'dictionary.dict')\n",
    "        dictionary.compactify()\n",
    "        dictionary.save(dictionary_save_path)\n",
    "        corpora.MmCorpus.serialize( os.path.join(data_folder,'corpus_bow.mm'), corpus_bow)\n",
    "        corpora.MmCorpus.serialize( os.path.join(data_folder,'corpus_tfidf.mm'), corpus_tfidf)\n",
    "        #print(len(dictionary))\n",
    "    return docs,dictionary,corpus_bow,corpus_tfidf\n",
    "\n",
    "#%%\n",
    "\n",
    "   \n",
    "def basic_lda(total_topics,corpus,dictionary,docs,score=False):\n",
    "    \n",
    "    print('Training for {} documents ......'.format(len(corpus)))\n",
    "    \n",
    "    lda = LdaModel(corpus = corpus,\n",
    "                              id2word = dictionary,\n",
    "                              num_topics = total_topics,\n",
    "                              alpha='auto',\n",
    "                              eta = 'auto',\n",
    "                              random_state = np.random.RandomState(seed =2))#,\n",
    "                              #workers = 20) #\n",
    "                              #iterations = 1000,\n",
    "    # Compute Coherence Score\n",
    "    if score:\n",
    "        print('calculating coherence socre for {} documents ......'.format(len(docs)))\n",
    "        coherence_model_lda = CoherenceModel(model=lda, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "        print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "        return lda,coherence_lda\n",
    "    \n",
    "    return lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load original text to look through later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of raw documents 142564\n"
     ]
    }
   ],
   "source": [
    "with open(raw_data_path,'r',encoding='utf8') as f:\n",
    "    raw_doc = f.readlines()\n",
    "    raw_doc = [l.strip(' \\n') for l in raw_doc if len(l)>50]\n",
    "\n",
    "print('Length of raw documents {}'.format(len(raw_doc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load lemmentized corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of lemmentized corpus: 142564\n",
      "Length of length of bag-of-word corpus: 142564\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rerun = True\n",
    "if rerun == True:                ## save gensim objects, corpus, dictionary, and lda model\n",
    "    mode = 'all'\n",
    "    docs,dictionary,corpus_bow,corpus_tfidf = prepare_data(data_processed_folder,save=False)\n",
    "    # corpus_bow = [c for c in corpus_bow_full if len(c)>0]\n",
    "    \n",
    "print('Length of length of bag-of-word corpus: {}'.format(len(corpus_bow)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter out paragraphs with <20 words or contain 'titles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of corpus without \"<Title>\" and has more than 20 words: 123908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. The Russian economy proved to be more resilient than expected to the dual shocks of lower oil prices and sanctions. Output fell sharply in 2015, by 2.8 percent (revised from an initial estimate of 3.7 percent) but stabilized in 2016, contracting by only 0.2 percent. The relatively modest response to the large external shocks reflects the authorities’ effective policy response—floating exchange rate, banking system liquidity support and capital injections, and limited fiscal stimulus coupled with restrictive incomes policies—and was enabled by robust buffers.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_doc_new = list()\n",
    "corpus_bow_new = list()\n",
    "docs_new = list()\n",
    "\n",
    "tuple_temp = [(x, y, z) for (x, y, z) in zip(raw_doc, corpus_bow, docs) if len(x.split())>20 and ('<Title>' not in x) ]\n",
    "\n",
    "raw_doc_new, corpus_bow_new, docs_new = zip(*tuple_temp)\n",
    "\n",
    "print('Length of corpus without \"<Title>\" and has more than 20 words: {}'.format(len(raw_doc_new)))\n",
    "\n",
    "raw_doc_new[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 1 LDA model and get results for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# n_topics = 40 # number of topics assumed\n",
    "# n_words = 20  # number of key words interested\n",
    "# np.random.seed(seed=1)\n",
    "# model, score = basic_lda(total_topics=n_topics,corpus=corpus_bow_new,dictionary=dictionary,docs=docs_new,score=True)\n",
    "\n",
    "# # View: given document, get its topics\n",
    "# doc_id = 0\n",
    "# tp = model.get_document_topics(bow= corpus_bow_new[doc_id], minimum_probability= 0.17)\n",
    "# print('tp: {}'.format(tp))\n",
    "# for i in tp:\n",
    "#     print('topic: {}'.format(i))\n",
    "#     print(model.show_topic(topicid=i[0],topn=n_words))\n",
    "\n",
    "# # Create a topic-key word table\n",
    "# topic_df = pd.DataFrame(data = np.zeros((n_topics, n_words)), columns= ['word'+ str(x) for x in range(n_words)])\n",
    "# for i in range(n_topics):\n",
    "#     topic_df.iloc[i] = pd.DataFrame(model.show_topic(topicid= i, topn= n_words))[0].tolist()\n",
    "\n",
    "# topic_df\n",
    "\n",
    "# # Now create a document-topic dataframe\n",
    "# docs_df = pd.DataFrame(data = np.zeros(len(docs_new)), columns=['paragraph'])\n",
    "# docs_df['paragraph'] = raw_doc_new\n",
    "\n",
    "# col_names = ['T'+ str(i) for i in np.array(range(n_topics))]\n",
    "# for col in col_names:\n",
    "#     docs_df[col]= 0 \n",
    "\n",
    "# docs_df.head()\n",
    "\n",
    "# for row in range(docs_df.shape[0]):\n",
    "#     tp = model.get_document_topics(bow= corpus_bow_new[row], minimum_probability= 0.2)\n",
    "#     for x in tp:\n",
    "#         docs_df.loc[row, 'T'+ str(x[0])] = x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Test: given topic, find most possible document\n",
    "# top_document_per_topic = []\n",
    "\n",
    "# for t_id in range(n_topics):\n",
    "#     t = 'T'+ str(t_id)\n",
    "#     print(\"Topic {}:\".format(t_id))\n",
    "#     print(model.show_topic(topicid= t_id, topn=n_words))\n",
    "#     print(str(raw_doc_new[docs_df[t].idxmax()]))\n",
    "#     top_document_per_topic.append(str(raw_doc_new[docs_df[t].idxmax()]))\n",
    "\n",
    "# # Make a table for better visualization\n",
    "\n",
    "# top_document_per_topic_df = pd.DataFrame(data = topic_df[topic_df.columns[0:]].apply(\n",
    "#     lambda x: ','.join(x.astype(str)), axis =1),\n",
    "#                                          columns =['topic'])\n",
    "# top_document_per_topic_df['top_document']= top_document_per_topic \n",
    "# top_document_per_topic_df\n",
    "\n",
    "# # save results to excel\n",
    "# now = datetime.datetime.now()\n",
    "# now = now.strftime(\"%Y_%m_%d\")\n",
    "# print(now)\n",
    "# writer = pd.ExcelWriter(path = os.path.join(results_folder,'Analysis_{}_topics_{}.xlsx'.format(n_topics, now)))\n",
    "# top_document_per_topic_df.to_excel(writer,'Topic and Top Document')\n",
    "\n",
    "# # save toipc-key word to excel\n",
    "# topic_df.to_excel(writer, 'Toipc and Key Word')\n",
    "\n",
    "# # save document-toipc mapping to excel\n",
    "# # transform from wide to long format\n",
    "# docs_df_long = pd.melt(docs_df,id_vars=['paragraph'], value_vars=['T'+ str(i) for i in range(50)] )\n",
    "# docs_df_long= docs_df_long[docs_df_long['value']>0]\n",
    "\n",
    "# docs_df_long.rename(columns={'variable':'topic','value':'probabiilty'}, inplace= True)\n",
    "# docs_df_long.topic = docs_df_long.topic.apply(lambda x: x.replace('T',''))\n",
    "# docs_df_long.to_excel(writer, 'Document and Topic')\n",
    "\n",
    "# writer.save()\n",
    "\n",
    "# # Create html visulaization using pyLDAvis\n",
    "# viz_data= pyLDAvis.gensim.prepare(model, corpus_bow_new, dictionary, sort_topics= False)\n",
    "# # pyLDAvis.prepare\n",
    "# #pyLDAvis.enable_notebook()\n",
    "# #pyLDAvis.display(viz_data)\n",
    "\n",
    "# #pyLDAvis.save_json(viz_data, '../../../analysis/Analysis_{}_topics_{}.js'.format(n_topics, now) )\n",
    "# pyLDAvis.save_html(viz_data,  os.path.join(results_folder,'Analysis_{}_topics_{}.html'.format(n_topics, now))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run LDA using Mallet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_topics = 40\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus_bow_new, num_topics=n_topics, id2word= dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## a better way to print \n",
    "def print_topics_gensim(topic_model, total_topics=1,\n",
    "                        weight_threshold=0.0001,\n",
    "                        display_weights=False,\n",
    "                        num_terms=None):\n",
    "    \n",
    "    for index in range(total_topics):\n",
    "        topic = topic_model.show_topic(index,topn=num_terms)\n",
    "        topic = [(word, round(wt,4)) \n",
    "                 for word, wt in topic \n",
    "                 if abs(wt) >= weight_threshold]\n",
    "        if display_weights:\n",
    "            print('Topic #'+str(index+1)+' with weights')\n",
    "            print (topic[:num_terms] if num_terms else topic)\n",
    "        else:\n",
    "            print ('Topic #'+str(index+1)+' without weights')\n",
    "            tw = [term for term, wt in topic]\n",
    "            print (tw[:num_terms] if num_terms else tw)\n",
    "        print()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating coherence socre for 123908 documents ......\n",
      "\n",
      "Coherence Score:  0.629796149683\n"
     ]
    }
   ],
   "source": [
    "print('calculating coherence socre for {} documents ......'.format(len(docs_new)))\n",
    "coherence_model_lda = CoherenceModel(model=ldamallet, texts=docs_new, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1 without weights\n",
      "['financial', 'financial_sector', 'institution', 'supervision', 'strengthen', 'system', 'regulatory', 'regulation', 'banking', 'stability']\n",
      "\n",
      "Topic #2 without weights\n",
      "['law', 'rule', 'establish', 'legal', 'legislation', 'governance', 'act', 'transparency', 'framework', 'issue']\n",
      "\n",
      "Topic #3 without weights\n",
      "['government', 'public', 'finance', 'level', 'central', 'transfer', 'local', 'include', 'state', 'general']\n",
      "\n",
      "Topic #4 without weights\n",
      "['sector', 'service', 'economy', 'market', 'competition', 'small', 'activity', 'good', 'industry', 'firm']\n",
      "\n",
      "Topic #5 without weights\n",
      "['problem', 'address', 'challenge', 'face', 'significant', 'concern', 'difficult', 'recent', 'give', 'constraint']\n",
      "\n",
      "Topic #6 without weights\n",
      "['suggest', 'impact', 'large', 'change', 'effect', 'estimate', 'potential', 'factor', 'gap', 'output']\n",
      "\n",
      "Topic #7 without weights\n",
      "['authority', 'note', 'agree', 'mission', 'stress', 'emphasize', 'importance', 'concern', 'recognize', 'point']\n",
      "\n",
      "Topic #8 without weights\n",
      "['management', 'assistance', 'strengthen', 'fund', 'improve', 'capacity', 'technical', 'plan', 'control', 'include']\n",
      "\n",
      "Topic #9 without weights\n",
      "['tax', 'revenue', 'administration', 'measure', 'reduce', 'vat', 'collection', 'custom', 'system', 'exemption']\n",
      "\n",
      "Topic #10 without weights\n",
      "['growth', 'recovery', 'demand', 'slow', 'strong', 'economy', 'activity', 'domestic', 'investment', 'expect']\n",
      "\n",
      "Topic #11 without weights\n",
      "['gdp', 'deficit', 'surplus', 'increase', 'year', 'estimate', 'table', 'project', 'decline', 'primary']\n",
      "\n",
      "Topic #12 without weights\n",
      "['bank', 'capital', 'loan', 'credit', 'banking_system', 'asset', 'lending', 'npls', 'deposit', 'exposure']\n",
      "\n",
      "Topic #13 without weights\n",
      "['country', 'trade', 'regional', 'agreement', 'export', 'region', 'tariff', 'good', 'integration', 'liberalization']\n",
      "\n",
      "Topic #14 without weights\n",
      "['development', 'poverty', 'social', 'reduction', 'support', 'strategy', 'program', 'education', 'poor', 'health']\n",
      "\n",
      "Topic #15 without weights\n",
      "['risk', 'global', 'shock', 'economy', 'crisis', 'impact', 'outlook', 'affect', 'adverse', 'downside']\n",
      "\n",
      "Topic #16 without weights\n",
      "['fiscal', 'adjustment', 'consolidation', 'medium-term', 'target', 'achieve', 'reduce', 'medium_term', 'sustainability', 'path']\n",
      "\n",
      "Topic #17 without weights\n",
      "['increase', 'household', 'credit', 'rise', 'high', 'income', 'low', 'rapid', 'mortgage', 'housing']\n",
      "\n",
      "Topic #18 without weights\n",
      "['inflation', 'pressure', 'price', 'increase', 'rise', 'remain', 'effect', 'demand', 'domestic', 'low']\n",
      "\n",
      "Topic #19 without weights\n",
      "['exchange_rate', 'real', 'competitiveness', 'broadly', 'term', 'suggest', 'line', 'reer', 'approach', 'appreciation']\n",
      "\n",
      "Topic #20 without weights\n",
      "['year', 'decline', 'average', 'past', 'half', 'rise', 'reflect', 'fall', 'recent', 'period']\n",
      "\n",
      "Topic #21 without weights\n",
      "['project', 'expect', 'current_account', 'external', 'export', 'import', 'decline', 'scenario', 'deficit', 'medium_term']\n",
      "\n",
      "Topic #22 without weights\n",
      "['monetary_policy', 'monetary', 'interest_rate', 'exchange_rate', 'central_bank', 'rate', 'liquidity', 'condition', 'intervention', 'instrument']\n",
      "\n",
      "Topic #23 without weights\n",
      "['financing', 'debt', 'arrears', 'domestic', 'external', 'payment', 'borrowing', 'fund', 'finance', 'term']\n",
      "\n",
      "Topic #24 without weights\n",
      "['program', 'performance', 'review', 'arrangement', 'structural', 'meet', 'target', 'authority', 'criterion', 'support']\n",
      "\n",
      "Topic #25 without weights\n",
      "['budget', 'expenditure', 'spending', 'revenue', 'increase', 'fiscal', 'bill', 'wage', 'cut', 'target']\n",
      "\n",
      "Topic #26 without weights\n",
      "['public', 'company', 'enterprise', 'privatization', 'restructuring', 'large', 'plan', 'include', 'state', 'operation']\n",
      "\n",
      "Topic #27 without weights\n",
      "['framework', 'important', 'ensure', 'role', 'provide', 'give', 'great', 'policy', 'future', 'place']\n",
      "\n",
      "Topic #28 without weights\n",
      "['reserve', 'international', 'foreign', 'currency', 'net', 'position', 'official', 'month', 'fx', 'payment']\n",
      "\n",
      "Topic #29 without weights\n",
      "['measure', 'give', 'support', 'current', 'additional', 'limit', 'policy', 'fiscal_policy', 'stimulus', 'fiscal']\n",
      "\n",
      "Topic #30 without weights\n",
      "['wage', 'employment', 'labor_market', 'increase', 'labor', 'unemployment', 'high', 'worker', 'work', 'job']\n",
      "\n",
      "Topic #31 without weights\n",
      "['political', 'early', 'follow', 'government', 'election', 'start', 'year', 'late', 'january', 'march']\n",
      "\n",
      "Topic #32 without weights\n",
      "['authority', 'action', 'include', 'plan', 'effort', 'measure', 'implement', 'strengthen', 'step', 'encourage']\n",
      "\n",
      "Topic #33 without weights\n",
      "['oil', 'price', 'production', 'energy', 'electricity', 'subsidy', 'increase', 'oil_price', 'cost', 'fuel']\n",
      "\n",
      "Topic #34 without weights\n",
      "['system', 'pension', 'benefit', 'cost', 'increase', 'saving', 'social', 'contribution', 'reform', 'reduce']\n",
      "\n",
      "Topic #35 without weights\n",
      "['remain', 'high', 'low', 'level', 'continue', 'strong', 'large', 'recent', 'reflect', 'significantly']\n",
      "\n",
      "Topic #36 without weights\n",
      "['market', 'domestic', 'security', 'bond', 'large', 'investor', 'foreign', 'stock', 'asset', 'spread']\n",
      "\n",
      "Topic #37 without weights\n",
      "['reform', 'progress', 'effort', 'make', 'improve', 'implementation', 'key', 'structural_reform', 'area', 'implement']\n",
      "\n",
      "Topic #38 without weights\n",
      "['data', 'account', 'national', 'improve', 'report', 'quality', 'statistic', 'improvement', 'economic', 'make']\n",
      "\n",
      "Topic #39 without weights\n",
      "['policy', 'macroeconomic', 'economic', 'stability', 'growth', 'maintain', 'discussion', 'focus', 'economy', 'challenge']\n",
      "\n",
      "Topic #40 without weights\n",
      "['investment', 'business', 'improve', 'infrastructure', 'private_sector', 'growth', 'environment', 'public', 'development', 'boost']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_topics_gensim(ldamallet,total_topics=n_topics,display_weights=False,num_terms=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def fine_tune_lda(corpus, dictionary, texts, limit, start=2, step=2):\n",
    "#     \"\"\"\n",
    "#     Compute c_v coherence for various number of topics\n",
    "#     Parameters:\n",
    "#     ----------\n",
    "#     dictionary : Gensim dictionary\n",
    "#     corpus : Gensim corpus\n",
    "#     texts : List of input texts\n",
    "#     limit : Max num of topics\n",
    "\n",
    "#     Returns:\n",
    "#     -------\n",
    "#     model_list : List of LDA topic models\n",
    "#     coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "#     n_topics : numbmber of topics\n",
    "#     \"\"\"\n",
    "#     coherence_values = []\n",
    "#     model_list = []\n",
    "#     n_topics = []\n",
    "#     for num_topics in range(start, limit, step):\n",
    "#         print('\\nTraing with n_topics = {}, training sample = {}.'.format(num_topics,len(corpus)))\n",
    "#         model = LdaModel(corpus = corpus,\n",
    "#                           id2word = dictionary,\n",
    "#                           random_state = 2,\n",
    "#                           alpha='auto',\n",
    "#                           eta = 'auto',\n",
    "#                           num_topics = num_topics)#\n",
    "#                           #distributed = True)  # alpha='auto' is not implenented in distributed lda\n",
    "#         model_list.append(model)\n",
    "#         print('Calculating coherence score based on {} samples.'.format(len(texts)))\n",
    "#         coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_values.append(coherencemodel.get_coherence())\n",
    "#         n_topics.append(num_topics)\n",
    "#         print(\"{}: {}\".format(num_topics,coherence_values[-1]))\n",
    "        \n",
    "\n",
    "#     return model_list, coherence_values,n_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #%%\n",
    "# if __name__== \"__main__\":\n",
    "    \n",
    "#     save = True  ## save gensim objects, corpus, dictionary, and lda model\n",
    "#     mode = 'all'\n",
    "#     docs,dictionary,corpus_bow,corpus_tfidf = prepare_data(save=save)\n",
    "#     corpus_bow = [c for c in corpus_bow if len(c)>0]\n",
    "    \n",
    "#     if mode == 'lda' or mode=='all':\n",
    "#         n_topics = 25\n",
    "#         model, score = basic_lda(total_topics=n_topics,corpus=corpus_bow,dictionary=dictionary,docs=docs,score=True)\n",
    "#         print(score)\n",
    "#         print_topics_gensim(topic_model=model,\n",
    "#                            total_topics = n_topics,\n",
    "#                            num_terms=20,\n",
    "#                            display_weights=True)\n",
    "#     if mode =='seed_lda' or mode=='all':\n",
    "#         n_topics = 25\n",
    "#         boost = 1000\n",
    "#         seed_topic_list = [['mpm','MPM','CFM','cfm','ltv','LTC','DSTI','dsti','lcr','LCR',\n",
    "#                             'capital_buffer','macroprudential','capital_flow','prudential'],\n",
    "#                             ['population','ageing','pension','productivity','migration','migrat']]\n",
    "            \n",
    "#         seed_model = seeded_lda(n_topics,corpus_bow,dictionary,docs,seed_topic_list, boost, score=False)\n",
    "#         ## for some reason keeps buging out when calculating coherence score \n",
    "        \n",
    "#         print_topics_gensim(topic_model=seed_model,\n",
    "#                            total_topics = n_topics,\n",
    "#                            num_terms=20,\n",
    "#                            display_weights=True)\n",
    "    \n",
    "#     if mode == 'fine_tune' or mode =='all':\n",
    "        \n",
    "#         model_list, coherence_values,n_topics = fine_tune_lda(dictionary=dictionary, corpus=corpus_bow,\n",
    "#                                                             texts=docs, start=15, limit=35, step=1)\n",
    "        \n",
    "#         best_model = model_list[np.argmax(coherence_values)]\n",
    "#         best_topic_n = best_model.get_topics().shape[0]\n",
    "        \n",
    "#         plt.plot(n_topics, coherence_values)\n",
    "#         plt.show()\n",
    "        \n",
    "#         print_topics_gensim(topic_model=best_model,\n",
    "#                        total_topics = best_topic_n,\n",
    "#                        num_terms=10,\n",
    "#                        display_weights=True)\n",
    "#         if save:\n",
    "#             lda_model_filepath = '../data/lda_res'\n",
    "#             best_model.save(lda_model_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def mallet_lda(model_path,total_topics,corpus,dictionary,docs,score=False):\n",
    "#     \"\"\"\n",
    "#     https://radimrehurek.com/gensim/models/wrappers/ldamallet.html\n",
    "#     sudo apt-get install default-jdk\n",
    "#     sudo apt-get install ant\n",
    "#     git clone git@github.com:mimno/Mallet.git\n",
    "#     cd Mallet/\n",
    "#     ant\n",
    "    \n",
    "#     we don't have those packages in server environment\n",
    "#     \"\"\"\n",
    "#     lda = gensim.models.wrappers.LdaMallet(model_path, corpus=corpus, num_topics=total_topics, id2word=dictionary)\n",
    "#     if score:\n",
    "#         print('calculating coherence socre for {} documents ......'.format(len(docs)))\n",
    "#         coherence_model = CoherenceModel(model=lda, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_score = coherence_model.get_coherence()\n",
    "#         print('\\nCoherence Score: ', coherence_score)\n",
    "#         return lda,coherence_score\n",
    "    \n",
    "# def hdp(corpus,dictionary,docs,score=False):\n",
    "#     print('Traiing for {} documents ......'.format(len(corpus)))\n",
    "#     hdpmodel = HdpModel(corpus = corpus,id2word = dictionary)\n",
    "#     if score:\n",
    "#         print('calculating coherence socre for {} documents ......'.format(len(docs)))\n",
    "#         coherence_model = CoherenceModel(model=hdpmodel, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_score = coherence_model.get_coherence()\n",
    "#         print('\\nCoherence Score: ', coherence_score)\n",
    "#         return hdpmodel,coherence_score\n",
    "#     return hdpmodel\n",
    "    \n",
    "# def lsi(total_topics, corpus,dictionary,docs,score=False):\n",
    "#     print('Traiing for {} documents ......'.format(len(corpus)))\n",
    "#     lsimodel = LsiModel(corpus = corpus,id2word = dictionary,num_topics=total_topics)\n",
    "#     if score:\n",
    "#         print('calculating coherence socre for {} documents ......'.format(len(docs)))\n",
    "#         coherence_model = CoherenceModel(model=lsimodel, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_score = coherence_model.get_coherence()\n",
    "#         print('\\nCoherence Score: ', coherence_score)\n",
    "#         return lsimodel,coherence_score\n",
    "#     return lsimodel\n",
    "\n",
    "# def seeded_lda(total_topics,corpus,dictionary,docs,seed_topic_list, boost, score=False):\n",
    "#     print('Modify beta prior ......')\n",
    "#     _model = LdaModel(corpus = corpus_bow, id2word = dictionary,random_state = 2,alpha='auto',num_topics = total_topics,iterations=0)\n",
    "#     beta_matrix = _model.expElogbeta\n",
    "#     for t_id, st in enumerate(seed_topic_list):\n",
    "#         for word in st:\n",
    "#             try:\n",
    "#                 w_id = dictionary.token2id[word]\n",
    "#                 beta_matrix[t_id,w_id] = boost\n",
    "#                 print('{} : {} : {}'.format(t_id,w_id,word))\n",
    "#             except:\n",
    "#                 continue\n",
    "#     print('Training for {} documents ......'.format(len(corpus)))\n",
    "#     seed_model = LdaModel(corpus = corpus_bow,\n",
    "#                                   id2word = dictionary,\n",
    "#                                   num_topics = total_topics,\n",
    "#                                   eta = beta_matrix,\n",
    "#                                   random_state=2)\n",
    "#     # Compute Coherence Score\n",
    "#     if score:\n",
    "#         print('calculating coherence socre for {} documents ......'.format(len(docs)))\n",
    "#         coherence_model_lda = CoherenceModel(model=seed_model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_lda = coherence_model_lda.get_coherence()\n",
    "#         print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "#         return seed_model,coherence_lda\n",
    "    \n",
    "#     return seed_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
