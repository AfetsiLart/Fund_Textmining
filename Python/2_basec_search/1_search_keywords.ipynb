{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Keywords Search \n",
    "- it is an simple example of how to search through document, find keyworkds, and get the paragraph context. \n",
    "- we will use all Staff Reports from 2000 - 2016. Most data came from COM xml database, but I also patched some missing years with IR documents. That is why you will see we are going to process two different data srouce in the code\n",
    "- a metadata sheet is also available to match all our documents with proper metadat info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### import some modules we gonna use \n",
    "import os \n",
    "python_root = '..'\n",
    "import sys\n",
    "sys.path.insert(0, python_root)\n",
    "import data_util\n",
    "import pickle\n",
    "import csv\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make sure you have pandas and nltk installed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Donload preprocessed data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In 1_process_xmls, we have shown one way of processing COM's xml data base. Here, I have already compiled a small database, with all article IV document i can possible find from 1990 to 2016.  \n",
    "- All documents has been preprocessed into a document object that we defined earlier in 1_process_xmls, detailed steps can be found in ../data_util. \n",
    "- From the link, you can download all preprocessed data and also raw data in case you want to process them in a different way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### specify download path and extract path \n",
    "download_path = \"staff_reports_full_sample.zip\"\n",
    "download_link = \"https://www.dropbox.com/s/23vdujvh67io2iz/staff_reports_full_sample.zip?dl=1\"\n",
    "extract_path = \"./data\"  # place data in Python project root folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "article iv: 589MB [02:31, 3.87MB/s]                                            \n"
     ]
    }
   ],
   "source": [
    "## detailed of the download_data function is in data_util module in python_root \n",
    "## if you do not yet have the data, run this code, it will set up a data folder under ./Python folder \n",
    "data_util.download_data(download_path,download_link,extract_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load xml documents and txt documents from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents:  2960\n",
      "\n",
      "One example: \n",
      "\n",
      "doc_id: 9781451821079 \n",
      "\n",
      "First paragraph: \n",
      "1. At the conclusion of the last Article IV consultation in March 2002 (SUR/02/33, 3/19/02), Directors commended Kenya for achieving a measure of macroeconomic stability during recent years in difficult circumstances. Directors, however, were concerned that the macroeconomic and financial situation remained fragile, and that investor confidence was low. They stressed the importance of implementing a comprehensive medium-term economic and structural reform program and undertaking measures to address the governance problems that had stalled progress to date. Directors stressed that it was important for Kenya to implement the prior actions needed to resume the Poverty Reduction and Growth Facility (PRGF)-supported program and to help restore confidence.1\n"
     ]
    }
   ],
   "source": [
    "## laod xml_dict and txt_dict \n",
    "xml_dict = pickle.load(open('data/xlm_docs.p', \"rb\")) \n",
    "txt_dict = pickle.load(open('data/txt_docs.p', \"rb\")) \n",
    "\n",
    "## merge them together \n",
    "# if you are running python 3.5 or later, you can use this one line to merge them \n",
    "doc_dict = {**xml_dict,**txt_dict}\n",
    "\n",
    "## print some information \n",
    "doc_ids = list(doc_dict.keys())\n",
    "print(\"number of documents: \", len(doc_dict))\n",
    "print(\"\\nOne example: \\n\\ndoc_id: {} \\n\\nFirst paragraph: \\n{}\".format(doc_ids[0],doc_dict[doc_ids[0]].paras[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get keywords, i am reading it from a txt file. Of course you can simply define it as a list in your script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:  ['asset quality', 'bad asset', 'bad debt', 'bad loan', 'balance sheet mismatch', 'balance-sheet mismatch', 'bank profitability', 'bankruptcies', 'bankruptcy', 'bond spread']\n"
     ]
    }
   ],
   "source": [
    "### get keyword list \n",
    "def read_keywords(file):\n",
    "    \"\"\"\n",
    "    file: csv file with keyword list\n",
    "    \"\"\"\n",
    "    with open(file,'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        mylist = list(reader)\n",
    "    return mylist\n",
    "\n",
    "key_file = 'search_keywords.csv'\n",
    "keywords = read_keywords(key_file)\n",
    "keywords = [k[0].replace('_',' ') for k in keywords] ## in keywords we used _ as [space], change it back \n",
    "print('Keywords: ', keywords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use regular expression to find keywords in your text \n",
    "- regular expression can get really complicated, we may have a stand alone tutorial for regular expression. \n",
    "- Here what we are trying to achieve is to feed in a list of keywords, and return a list of keywords found, and also the frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define a function to loacte keywords\n",
    "def construct_rex(keywords):\n",
    "    r_keywords = [r'\\b' + re.escape(k) + r'(s|es)?\\b'for k in keywords]    # tronsform keyWords list to a patten list, find both s and es \n",
    "    rex = re.compile('|'.join(r_keywords),flags=re.I)                        # use or to join all of them, ignore casing\n",
    "    #match = [(m.start(),m.group()) for m in rex.finditer(content)]          # get the position and the word\n",
    "    return rex\n",
    "\n",
    "def find_exact_keywords(content,keywords,rex=None):\n",
    "    if rex is None: \n",
    "        rex = construct_rex(keywords)\n",
    "    content = content.replace('\\n', '').replace('\\r', '')#.replace('.',' .')\n",
    "    match = Counter([m.group() for m in rex.finditer(content)])             # get all instances of matched words \n",
    "                                                                             # and turned them into a counter object, to see frequencies\n",
    "    return match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's see how ti works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: \n",
      "10. Remarkable progress has been made in consolidating the peace process culminating in the agreement signed in Pretoria on December 17,2002. In line with the Lusaka accords of 1999, peace agreements were signed with Rwanda (end-July 2002) and Uganda (early September 2002), and these countries have all but completed the withdrawal of their troops. On the government side, Angola is also completing the withdrawal of its troops, while Namibia and Zimbabwe have already done so. On November 11, 2002, Presidents Kabila and Kagame agreed to extend the period envisaged in the original peace agreement by three months to allow for the disarming and repatriation of ex-Rwandan Hutu soldiers. Meanwhile, Phase III of the United Nations Observation Mission (MONUC) to the DRC continues and the regional demobilization and reintegration program is gradually being put in place, with the help of the United Nations (UN) and the World Bank. On December 5, 2002, the UN Security Council passed Resolution 1445, raising the number of authorized peacekeeping troops assigned to MONUC from 5,500 to 8,700 (some 5,000 MONUC troops were stationed in the DRC as of end-2002). This will help fill the security vacuum that has emerged from the withdrawal of foreign troops, particularly in the eastern region, with tragic humanitarian consequences. The authorities noted that they had tried to remedy the situation by increasing security-related expenditure beyond what had been envisaged in the program.\n",
      "\n",
      "keywords found: \n",
      " Counter()\n"
     ]
    }
   ],
   "source": [
    "content = doc_dict[doc_ids[100]].paras[9]\n",
    "print(\"Content: \\n{}\".format(content))\n",
    "rex = construct_rex(keywords)\n",
    "results = find_exact_keywords(content.lower(),keywords,rex)\n",
    "print(\"\\nkeywords found: \\n {}\".format(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's loop through all document we have and put all keywords frequencies together\n",
    "- depends on how many documents you have, this may take a while \n",
    "- you are more then welcome to modify this to seep it up. Depends on your need, if all you care about is a document level statistics, you can concatinate all paragraphs and do one regex search, it will be much faster. \n",
    "- We are doing it in paragaph level, just in case you will need paragraph context in the future. \n",
    "- you can also try multi thread it, if time is really precious to you. You can find some usefuly information here: https://pymotw.com/2/multiprocessing/basics.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2960\n",
      "100/2960\n",
      "200/2960\n",
      "300/2960\n"
     ]
    }
   ],
   "source": [
    "total_doc_num = len(doc_dict.items())\n",
    "df = pd.DataFrame()               ## define a empty data frame, will populat it as we loop though documents\n",
    "rex = construct_rex(keywords)     ## construct regular expression \n",
    "\n",
    "for idx,(ite) in enumerate(doc_dict.items()):\n",
    "    #if idx > 500: break\n",
    "    key,doc = ite     ## get id and doc\n",
    "    val = doc.paras   ## read all paragraphs \n",
    "    if len(val)==0:   ## skip if document has no paragraph \n",
    "        continue\n",
    "    for idxp,content in enumerate(val):                                    ## loop though each paragaph in a document \n",
    "        results = find_exact_keywords(content.lower(),keywords,rex)        ## get keywords frequency in each paragraph \n",
    "        if len(results) == 0:                                              ## if nothing found, skip \n",
    "            continue\n",
    "        results['context'] = content                                    ## if keyword found, assign context to be the paragaph\n",
    "        results['doc_id'] = str(key)                                    ## assign documetn id \n",
    "        results['para_id'] = str(key) + '_' + str(idxp)                 ## assigin paragaph id, we may not use this at all, but just in case\n",
    "        results['para_word_count'] = len(content.split())               ## get paragaph word count \n",
    "        df = df.append(results, ignore_index=True)\n",
    "    \n",
    "    if idx%100 == 0 :                                     ## print every 100 iterations \n",
    "        print('{}/{}'.format(idx,total_doc_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## order doc id and context to first 3 columns \n",
    "keys = df.columns\n",
    "meta = ['doc_id','para_id','context','para_word_count']\n",
    "keys = [i for i in keys if i not in meta]\n",
    "meta.extend(keys)\n",
    "df = df[meta]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save results to csv \n",
    "df.to_csv('data/search_results.csv',encoding='utf-8')\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, every row is a paragraph, with keyword frequencies. in most cases, we would want country level aggregated data. Please see follow up notebook for data aggregation and visualization. \n",
    "\n",
    "##### alternatively, you can also simply import all these data into stata, and aggregate them in stata. I have attached a stata do file, for your reference. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
